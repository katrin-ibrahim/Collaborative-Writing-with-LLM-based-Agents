# FILE: runners/runner_utils.py
import sys
from pathlib import Path

import logging
import re
from typing import List

# Add src directory to path
src_dir = Path(__file__).parent.parent
if str(src_dir) not in sys.path:
    sys.path.insert(0, str(src_dir))

from config.baselines_model_config import ModelConfig
from utils.data_models import Article
from utils.ollama_client import OllamaClient

from .llm_wrapper import OllamaLiteLLMWrapper

logger = logging.getLogger(__name__)


def get_model_wrapper(
    client: OllamaClient, config: ModelConfig, task: str
) -> OllamaLiteLLMWrapper:
    model = config.get_model_for_task(task)
    temp = config.get_temperature_for_task(task)
    max_tokens = config.get_token_limit_for_task(task)
    return OllamaLiteLLMWrapper(
        ollama_client=client, model=model, temperature=temp, max_tokens=max_tokens
    )


def extract_storm_output_ollama(topic, storm_output_dir: str):
    """
    Extract STORM output content and metadata from output files.

    Args:
        topic: Topic string or FreshWikiEntry object
        storm_output_dir: Directory with STORM output

    Returns:
        tuple: (content, storm_metadata)
    """
    # Extract topic string if it's a FreshWikiEntry object
    topic_str = topic.topic if hasattr(topic, "topic") else str(topic)

    # Handle directory path
    topic_dir = Path(storm_output_dir) / topic_str.replace(" ", "_").replace("/", "_")
    output_files = [
        "storm_gen_article_polished.txt",
        "storm_gen_article.txt",
        "storm_gen_outline.txt",
    ]

    logger.debug(f"Looking for STORM output in: {topic_dir}")
    logger.debug(f"Directory exists: {topic_dir.exists()}")

    if topic_dir.exists():
        logger.debug(f"Directory contents: {list(topic_dir.iterdir())}")

    for filename in output_files:
        path = topic_dir / filename
        logger.debug(f"Checking file: {path}")
        logger.debug(f"File exists: {path.exists()}")

        if path.exists():
            try:
                content = path.read_text(encoding="utf-8")
                logger.debug(f"File content length: {len(content)}")
                logger.debug(f"Content preview: {content[:200]}...")

                if content.strip():
                    logger.info(f"Using STORM output from: {filename}")
                    # Return tuple of content and metadata
                    return content, {"source_file": filename}
                else:
                    logger.warning(f"File {filename} is empty")
            except Exception as e:
                logger.warning(f"Could not read {path}: {e}")
        else:
            logger.debug(f"File does not exist: {path}")

    logger.error(f"No valid STORM output found for topic: {topic}")
    logger.error(f"Searched in directory: {topic_dir}")
    return f"# {topic}\n\nNo content generated by STORM.", {
        "error": "No content files found"
    }


def log_result(method: str, article: Article):
    if "error" in article.metadata:
        logger.warning(f"✗ {method} failed")
    else:
        wc = article.metadata.get("word_count", 0)
        logger.info(f"✓ {method} completed ({wc} words)")


def generate_search_queries(
    client, model_config, topic: str, num_queries: int = 5
) -> List[str]:
    """Generate diverse, high-quality search queries for retrieval."""
    prompt = f"""Generate {num_queries} strategic search queries to comprehensively research "{topic}".

You are a research librarian creating targeted search queries. Each query should:
1. Focus on a distinct aspect of the topic
2. Use specific terminology and keywords
3. Be optimized for finding factual, detailed information
4. Cover different temporal periods (historical, current, future)
5. Include both general and specific subtopics

For the topic "{topic}", create queries that would find:
- Historical background and origins
- Key people, organizations, or entities involved
- Technical details, processes, or mechanisms
- Current status, recent developments, or statistics
- Impact, significance, or broader implications

Format: One query per line, no numbering or prefixes.

Examples for reference:
- "origins history foundation established"
- "key figures leaders founders important people"
- "technical specifications details process how works"
- "current status recent developments 2023 2024"
- "impact significance effects implications"

Search queries for "{topic}":"""

    wrapper = get_model_wrapper(client, model_config, "fast")
    response = wrapper(prompt)

    if hasattr(response, "choices") and response.choices:
        content = response.choices[0].message.content
    elif hasattr(response, "content"):
        content = response.content
    else:
        content = str(response)

    queries = [q.strip() for q in content.split("\n") if q.strip()]
    # Clean up queries (remove numbering, bullets, etc.)
    cleaned_queries = []
    for q in queries:
        q = q.strip()
        # Remove common prefixes
        q = re.sub(r"^\d+[\.\)]\s*", "", q)
        q = re.sub(r"^[-•*]\s*", "", q)
        q = re.sub(r'^"([^"]*)"$', r"\1", q)  # Remove quotes
        if len(q) > 5 and q not in cleaned_queries:
            cleaned_queries.append(q)

    # Fallback queries if generation failed
    if not cleaned_queries:
        cleaned_queries = [
            f"{topic} history background origins",
            f"{topic} key people important figures",
            f"{topic} technical details specifications",
            f"{topic} current status recent developments",
            f"{topic} impact significance effects",
        ]

    return cleaned_queries[:num_queries]


def retrieve_and_format_passages(retrieval_system, queries: List[str]) -> List[dict]:
    """Retrieve passages for queries and format consistently."""
    all_passages = []

    for query in queries:
        try:
            passages = retrieval_system.retrieve(query)
            for passage in passages:
                all_passages.append(
                    {
                        "content": passage.get("content", ""),
                        "title": passage.get("title", ""),
                        "relevance": passage.get("score", 0.5),
                    }
                )
        except Exception as e:
            logger.warning(f"Retrieval failed for query '{query}': {e}")

    return all_passages


def create_context_from_passages(passages: List[dict], max_passages: int = 8) -> str:
    """Create enhanced context string from retrieved passages."""
    if not passages:
        return ""

    # Sort by relevance and take top passages
    sorted_passages = sorted(
        passages, key=lambda x: x.get("relevance", 0), reverse=True
    )

    # Increase max_passages for better coverage
    top_passages = sorted_passages[:max_passages]

    # Enhanced duplicate removal based on content similarity
    unique_passages = []
    for passage in top_passages:
        content = passage.get("content", "").strip()
        if not content:
            continue

        # Check for substantial overlap with existing passages
        is_duplicate = False
        for existing in unique_passages:
            existing_content = existing.get("content", "")
            # Simple overlap check - could be enhanced with semantic similarity
            if len(content) > 0 and len(existing_content) > 0:
                shorter_len = min(len(content), len(existing_content))
                longer_len = max(len(content), len(existing_content))
                # If one passage is substantially contained in another, skip it
                if shorter_len / longer_len > 0.8 and (
                    content in existing_content or existing_content in content
                ):
                    is_duplicate = True
                    break

        if not is_duplicate:
            unique_passages.append(passage)

    # Enhanced context formatting with better structure
    context_parts = []
    for i, passage in enumerate(unique_passages, 1):
        title = passage.get("title", "Unknown Source")
        content = passage.get("content", "")
        if content:
            # Clean up content (remove excessive whitespace, etc.)
            content = re.sub(r"\s+", " ", content).strip()
            # Limit individual passage length to prevent overwhelming context
            if len(content) > 800:
                content = content[:800] + "..."
            context_parts.append(f"[Source {i} - {title}]:\n{content}")

    context = "\n\n".join(context_parts)
    logger.debug(
        f"Created context with {len(unique_passages)} unique passages, {len(context)} chars"
    )

    return context


def generate_article_with_context(
    client, model_config, topic: str, context: str = ""
) -> str:
    """Generate article using retrieved context with enhanced integration."""
    if context:
        prompt = f"""You are an expert encyclopedia writer. Write a comprehensive Wikipedia-style article about "{topic}" using the provided research information.

RESEARCH INFORMATION:
{context}

WRITING INSTRUCTIONS:
1. Synthesize information from multiple sources to create a coherent narrative
2. Start with "# {topic}" as the main title
3. Create 4-6 well-structured sections with specific, descriptive headings
4. Each section should be 2-3 substantial paragraphs
5. Include specific facts, dates, numbers, and names from the research
6. Use proper citations format [1], [2], etc. when referencing sources
7. Maintain encyclopedic tone and neutral point of view
8. Ensure logical flow between sections
9. Include entity-rich content with proper nouns and technical terms
10. Aim for 1200-1600 words total

SECTION STRATEGY:
- Begin with a comprehensive introduction (2-3 paragraphs)
- Choose domain-specific section headings based on the topic type
- Prioritize the most important aspects covered in the research
- End with current status, impact, or future developments

QUALITY REQUIREMENTS:
- Every paragraph should contain specific, verifiable information
- Use transitional phrases to connect ideas smoothly
- Include quantitative data where available
- Mention key stakeholders, organizations, or individuals
- Reference geographic locations, time periods, and technical specifications

Write the article now:"""
    else:
        prompt = f"""Write a comprehensive Wikipedia-style article about "{topic}".

You are an expert encyclopedia writer. Create a detailed, factual article without external sources.

INSTRUCTIONS:
1. Start with "# {topic}" as the main title
2. Write a comprehensive introduction explaining the topic's significance
3. Create 4-6 sections with specific, descriptive headings (not generic ones)
4. Each section should contain 2-3 substantial paragraphs
5. Include plausible specific details, dates, and examples
6. Use encyclopedic tone and professional writing style
7. Aim for 1200-1600 words total
8. Focus on entity-rich content with proper nouns and technical terms

Write the article now:"""

    wrapper = get_model_wrapper(client, model_config, "writing")
    response = wrapper(prompt)

    if hasattr(response, "choices") and response.choices:
        content = response.choices[0].message.content
    elif hasattr(response, "content"):
        content = response.content
    else:
        content = str(response)

    # Ensure proper title format
    if not content.strip().startswith("#"):
        content = f"# {topic}\n\n{content}"

    return content
