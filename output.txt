2025-06-10 23:13:41,979 - __main__ - INFO - üå©Ô∏è  Enhanced STORM Baselines with Configuration
2025-06-10 23:13:41,980 - __main__ - INFO - Provider preference: together
2025-06-10 23:13:41,980 - __main__ - INFO - Methods: storm
2025-06-10 23:13:41,980 - __main__ - INFO - Configuration: storm_config.yaml
2025-06-10 23:13:41,980 - __main__ - INFO - Delay between topics: 30.0s
2025-06-10 23:13:41,980 - __main__ - INFO - Available providers: together, groq, huggingface
2025-06-10 23:13:41,980 - __main__ - INFO - Selected provider: together (meta-llama/Llama-3.2-3B-Instruct-Turbo)
2025-06-10 23:13:41,980 - __main__ - INFO - STORM settings: conv_turns=2, perspectives=2, retries=2
2025-06-10 23:13:45,283 - __main__ - INFO - Using together provider with model: meta-llama/Llama-3.2-3B-Instruct-Turbo
2025-06-10 23:13:45,304 - __main__ - INFO - Testing STORM configuration...

[1;31mProvider List: https://docs.litellm.ai/docs/providers[0m

2025-06-10 23:13:45,305 - LiteLLM - INFO - 
LiteLLM completion() model= meta-llama/Llama-3.2-3B-Instruct-Turbo; provider = None

[1;31mGive Feedback / Get Help: https://github.com/BerriAI/litellm/issues/new[0m
LiteLLM.Info: If you need to debug this error, use `litellm.set_verbose=True'.

2025-06-10 23:13:45,311 - __main__ - WARNING - Attempt 1/3 failed with together: litellm.APIConnectionError: Unable to map your input to a model. Check your input - {'model': 'meta-llama/Llama-3.2-3B-Instruct-Turbo', 'messages': [{'role': 'user', 'content': 'Test'}], 'timeout': 60, 'temperature': 0.7, 'top_p': 0.9, 'n': None, 'stream': None, 'stream_options': None, 'stop': None, 'max_completion_tokens': None, 'max_tokens': 10, 'modalities': None, 'prediction': None, 'audio': None, 'presence_penalty': None, 'frequency_penalty': None, 'logit_bias': None, 'user': None, 'response_format': None, 'seed': None, 'tools': None, 'tool_choice': None, 'logprobs': None, 'top_logprobs': None, 'parallel_tool_calls': None, 'deployment_id': None, 'extra_headers': None, 'functions': None, 'function_call': None, 'base_url': None, 'api_version': None, 'api_key': '2411327ef45aef514d4fa7c07914b855bba03b8b42b108c8e0b31564cd0bf0b3', 'model_list': None, 'kwargs': {'cache': {'no-cache': False, 'no-store': False}, 'api_base': 'https://api.together.xyz/v1', 'litellm_call_id': '391aace1-6c35-4d97-b0ab-11f7a7bfb83c', 'litellm_logging_obj': <litellm.litellm_core_utils.litellm_logging.Logging object at 0x3111697d0>}}
Traceback (most recent call last):
  File "/Users/katrin/Documents/Repos/Collaborative-Writing-with-LLM-based-Agents/venv/lib/python3.11/site-packages/litellm/main.py", line 2963, in completion
    raise ValueError(
ValueError: Unable to map your input to a model. Check your input - {'model': 'meta-llama/Llama-3.2-3B-Instruct-Turbo', 'messages': [{'role': 'user', 'content': 'Test'}], 'timeout': 60, 'temperature': 0.7, 'top_p': 0.9, 'n': None, 'stream': None, 'stream_options': None, 'stop': None, 'max_completion_tokens': None, 'max_tokens': 10, 'modalities': None, 'prediction': None, 'audio': None, 'presence_penalty': None, 'frequency_penalty': None, 'logit_bias': None, 'user': None, 'response_format': None, 'seed': None, 'tools': None, 'tool_choice': None, 'logprobs': None, 'top_logprobs': None, 'parallel_tool_calls': None, 'deployment_id': None, 'extra_headers': None, 'functions': None, 'function_call': None, 'base_url': None, 'api_version': None, 'api_key': '2411327ef45aef514d4fa7c07914b855bba03b8b42b108c8e0b31564cd0bf0b3', 'model_list': None, 'kwargs': {'cache': {'no-cache': False, 'no-store': False}, 'api_base': 'https://api.together.xyz/v1', 'litellm_call_id': '391aace1-6c35-4d97-b0ab-11f7a7bfb83c', 'litellm_logging_obj': <litellm.litellm_core_utils.litellm_logging.Logging object at 0x3111697d0>}}

2025-06-10 23:13:45,311 - __main__ - INFO - Switching to groq provider
2025-06-10 23:13:47,324 - __main__ - INFO - Testing STORM configuration...
2025-06-10 23:13:47,325 - LiteLLM - INFO - 
LiteLLM completion() model= llama-3.1-8b-instant; provider = groq
2025-06-10 23:13:47,729 - LiteLLM - INFO - Wrapper: Completed Call, calling success_handler
2025-06-10 23:13:47,733 - __main__ - INFO - ‚úì STORM initialized successfully with groq
2025-06-10 23:13:47,733 - __main__ - INFO - Loading topics from FreshWiki...
2025-06-10 23:13:47,736 - evaluation.benchmarks.freshwiki_loader - INFO - Found 19 FreshWiki entries with both JSON and TXT files
2025-06-10 23:13:47,744 - evaluation.benchmarks.freshwiki_loader - INFO - Successfully loaded 19 FreshWiki evaluation entries
2025-06-10 23:13:47,744 - __main__ - INFO - FreshWiki validation: {'status': 'loaded', 'total_entries': 19, 'topics_with_outlines': 19, 'topics_with_content': 19, 'topics_with_sections': 0, 'average_outline_length': 1.0, 'average_content_length': 2.5789473684210527, 'sample_topics': [{'topic': '# Aftermath', 'word_count': 2, 'outline_sections': 1}, {'topic': '[13] https://www.reuters.com/graphics/INDIA-CRASH/RAIL-INVESTIGATION/dwvkdwnbkpm/', 'word_count': 2, 'outline_sections': 1}, {'topic': '# Crash', 'word_count': 2, 'outline_sections': 1}, {'topic': '[20] https://apnews.com/article/india-train-derailment-odisha-f272d8811ed7826ea2fb110b8eb26441', 'word_count': 2, 'outline_sections': 1}, {'topic': '[27] https://indianexpress.com/article/india/odisha-train-tragedy-school-becomes-morgue-8644157/', 'word_count': 2, 'outline_sections': 1}, {'topic': '[8] https://www.bbc.com/news/world-asia-india-65796173', 'word_count': 2, 'outline_sections': 1}, {'topic': '[19] https://www.bbc.com/news/world-asia-india-65793257', 'word_count': 2, 'outline_sections': 1}, {'topic': '[3] https://www.bbc.com/news/world-asia-india-65801807', 'word_count': 2, 'outline_sections': 1}, {'topic': '2023_Odisha_train_collision', 'word_count': 1, 'outline_sections': 1}, {'topic': '# Victims', 'word_count': 2, 'outline_sections': 1}], 'word_count_distribution': {'<50': 19, '50-100': 0, '100-500': 0, '500+': 0}}
2025-06-10 23:13:47,744 - evaluation.benchmarks.freshwiki_loader - WARNING - No high-quality entries found, using all 19 available entries
2025-06-10 23:13:47,744 - evaluation.benchmarks.freshwiki_loader - INFO - Found 19 quality entries out of 19 total
2025-06-10 23:13:47,744 - __main__ - INFO - Loaded 1 topics for evaluation
2025-06-10 23:13:47,744 - __main__ - INFO -   1. [3] https://www.bbc.com/news/world-asia-india-65801807 (2 words, 1 sections)
2025-06-10 23:13:47,744 - ArticleEvaluator - INFO - ArticleEvaluator initialized with modular metrics
2025-06-10 23:13:47,744 - __main__ - INFO - Evaluation enabled
2025-06-10 23:13:47,744 - __main__ - INFO - 
============================================================
2025-06-10 23:13:47,744 - __main__ - INFO - Processing 1/1: [3] https://www.bbc.com/news/world-asia-india-65801807
2025-06-10 23:13:47,744 - __main__ - INFO - ============================================================
2025-06-10 23:13:47,745 - __main__ - INFO - üîÑ Running full STORM pipeline...
2025-06-10 23:13:47,745 - __main__ - INFO - Running STORM (attempt 1/2)...
2025-06-10 23:13:52,464 - LiteLLM - INFO - 
LiteLLM completion() model= llama-3.1-8b-instant; provider = groq
2025-06-10 23:13:52,953 - LiteLLM - INFO - Wrapper: Completed Call, calling success_handler
2025-06-10 23:13:52,956 - LiteLLM - INFO - 
LiteLLM completion() model= llama-3.1-8b-instant; provider = groq
2025-06-10 23:13:53,354 - LiteLLM - INFO - Wrapper: Completed Call, calling success_handler
2025-06-10 23:13:54,130 - LiteLLM - INFO - 
LiteLLM completion() model= llama-3.1-8b-instant; provider = groq
2025-06-10 23:14:24,903 - LiteLLM - INFO - Wrapper: Completed Call, calling success_handler
2025-06-10 23:14:24,905 - LiteLLM - INFO - 
LiteLLM completion() model= llama-3.1-8b-instant; provider = groq

[1;31mGive Feedback / Get Help: https://github.com/BerriAI/litellm/issues/new[0m
LiteLLM.Info: If you need to debug this error, use `litellm.set_verbose=True'.


[1;31mProvider List: https://docs.litellm.ai/docs/providers[0m

2025-06-10 23:15:24,936 - __main__ - WARNING - STORM attempt 1 failed: litellm.APIError: APIError: GroqException - litellm.Timeout: Connection timed out after None seconds.
2025-06-10 23:15:24,937 - __main__ - INFO - Waiting 12.7s before retry...
2025-06-10 23:15:37,608 - __main__ - INFO - Running STORM (attempt 2/2)...
2025-06-10 23:15:37,608 - __main__ - INFO - Adding 15.8s delay to reduce rate limiting...
2025-06-10 23:15:54,227 - LiteLLM - INFO - 
LiteLLM completion() model= llama-3.1-8b-instant; provider = groq
2025-06-10 23:16:25,018 - LiteLLM - INFO - Wrapper: Completed Call, calling success_handler
2025-06-10 23:16:25,022 - LiteLLM - INFO - 
LiteLLM completion() model= llama-3.1-8b-instant; provider = groq
2025-06-10 23:16:55,841 - LiteLLM - INFO - Wrapper: Completed Call, calling success_handler
2025-06-10 23:16:55,842 - LiteLLM - INFO - 
LiteLLM completion() model= llama-3.1-8b-instant; provider = groq
2025-06-10 23:16:56,527 - LiteLLM - INFO - Wrapper: Completed Call, calling success_handler
2025-06-10 23:16:56,529 - LiteLLM - INFO - 
LiteLLM completion() model= llama-3.1-8b-instant; provider = groq
2025-06-10 23:16:57,172 - LiteLLM - INFO - Wrapper: Completed Call, calling success_handler
2025-06-10 23:16:58,399 - primp - INFO - response: https://lite.duckduckgo.com/lite/ 200
2025-06-10 23:17:00,344 - primp - INFO - response: https://html.duckduckgo.com/html 200
2025-06-10 23:17:01,880 - primp - INFO - response: https://html.duckduckgo.com/html 200
2025-06-10 23:17:01,883 - LiteLLM - INFO - 
LiteLLM completion() model= llama-3.1-8b-instant; provider = groq
2025-06-10 23:17:02,703 - LiteLLM - INFO - Wrapper: Completed Call, calling success_handler
2025-06-10 23:17:02,712 - LiteLLM - INFO - 
LiteLLM completion() model= llama-3.1-8b-instant; provider = groq
2025-06-10 23:17:03,419 - LiteLLM - INFO - Wrapper: Completed Call, calling success_handler
2025-06-10 23:17:03,420 - LiteLLM - INFO - 
LiteLLM completion() model= llama-3.1-8b-instant; provider = groq
2025-06-10 23:17:34,164 - LiteLLM - INFO - Wrapper: Completed Call, calling success_handler
2025-06-10 23:17:34,166 - LiteLLM - INFO - 
LiteLLM completion() model= llama-3.1-8b-instant; provider = groq
2025-06-10 23:17:34,849 - LiteLLM - INFO - Wrapper: Completed Call, calling success_handler
2025-06-10 23:17:35,905 - primp - INFO - response: https://html.duckduckgo.com/html 200
2025-06-10 23:17:37,693 - primp - INFO - response: https://lite.duckduckgo.com/lite/ 200
2025-06-10 23:17:39,359 - primp - INFO - response: https://lite.duckduckgo.com/lite/ 200
2025-06-10 23:17:39,363 - LiteLLM - INFO - 
LiteLLM completion() model= llama-3.1-8b-instant; provider = groq

[1;31mGive Feedback / Get Help: https://github.com/BerriAI/litellm/issues/new[0m
LiteLLM.Info: If you need to debug this error, use `litellm.set_verbose=True'.


[1;31mProvider List: https://docs.litellm.ai/docs/providers[0m

2025-06-10 23:23:32,346 - root - ERROR - Error occurs when generating answer: litellm.APIError: APIError: GroqException - litellm.Timeout: Connection timed out after None seconds.
2025-06-10 23:23:32,349 - knowledge_storm.interface - INFO - run_knowledge_curation_module executed in 458.9361 seconds
2025-06-10 23:23:32,350 - LiteLLM - INFO - 
LiteLLM completion() model= llama-3.1-8b-instant; provider = groq
